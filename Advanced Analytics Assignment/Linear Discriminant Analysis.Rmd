---
title: "Linear Dicriminant Analysis"
author: "Karthick"
date: "31 March 2018"
output: html_document
---

###5.Linear Discriminant Analysis

#####5.1 The Model

The LDA Model behaves the same way as of the Logistic regression model. All the variables in the dataset are being used in LDA. 

The posterior probability of the each of the group (0 and 1) of the bad credit is calculated and and the one with the ***Higher Posterior Probability*** is used as the prediction of the model.

The ROC curve is constructed with the help of the posterior probability of Group 1 of bad credit ., replicating the response P(y=1|X=x) as in logistic regression.

Assumptions made :

The observation from each class are drawn from a gaussian distribution.

The LDA model uses Baye's theorem to perform prediction by plugging in estimates from the distribution.

```{r }
# Importing the dataset
dfFinal <- read.csv("dfFinal.csv")
dfFinal$totDep <- as.factor(dfFinal$totDep)
dfFinal$PHON <- as.factor(dfFinal$PHON)
dfFinal$bad <- as.factor(dfFinal$bad)

dataset = dfFinal
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$bad, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

library(MASS)
lda.fit = lda(bad~ ., data = training_set)

# Predicting the Test set results

lda.pred = as.data.frame(predict(lda.fit,test_set[-8]))
lda.prob = lda.pred$posterior.1

y_pred_lda = lda.pred$class
# Making the Confusion Matrix
cm = table(test_set[,8], lda.pred$class)
cm
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
sensitivity=(cm[2,2]/(cm[2,2]+cm[2,1]))
specificity=(cm[1,1]/(cm[1,1]+cm[1,2]))

print("the accuracy of the model is ")
print(accuracy)

print("the sensitivity of the model is ")
print(sensitivity)

print("the specificity of the model is ")
print(specificity)

library(pROC)
plot.roc(test_set[,8],lda.prob,print.auc=TRUE)
```
Linear Discriminant Analysis finds the mean and variance of the two distribution of the level of bad credit and use them to find which one has the higher probabilities given the values of X.

```{r }
plot(lda.fit)
```

#####5.2 Model Explanation

Summary of the model

```{r }
lda.fit
```

For the ease of interpretation:

consider 2 variables Age and netFamIncome,

LDA output indicates that prob(1) = 0.26 and prob(0) = 0.74, which are the prior probabilities.

The provides the average of the group means: the average of the each predictor within each class and are being used by LDA as estimates of mean.


***Group means***:

This suggest that the one with the bad credit has an age on average as 42 and income as 18000.

***Co-efficient of Linear Discriminants***

This outputs the linear combination of all the variables that are used to form the LDA decision rule. In other words these are the multipliers of the elements of the X's. For eg

LDA1 = 3.756977e-02 * Age   -4.097249e-05 (netFamIncome)

The number of LDA formed = (k-1) levels of the response 

In our case we have one LDA which has different combination of the variable to form the decision rule.

>This LDA is saved and used in the further analysis to improve the predictive power.

#####5.3 Cross validation - 10 Fold

```{r }
# Applying k-Fold Cross Validation
# install.packages('caret')
library(caret)
folds = createFolds(training_set$bad, k = 10)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifierLRM = glm(formula = bad ~ . - totDep -homeVal-AES-PHON,
                      family = binomial,
                      data = training_set)
  prob_pred = predict(classifierLRM, type = 'response', newdata = test_fold[-8])
  y_pred = ifelse(prob_pred > 0.5, 1, 0)
  cm = table(test_fold[, 8], y_pred > 0.5)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
  
})
accuracyLRM = mean(as.numeric(cv))
```

#####5.4 Quadratic Discriminant Analysis

LDA and QDA are more alike. But the advantage of using QDA over LDA is that ., QDA assumes that each clas has its own covariance matrix.

It is in the form of X ~ N( mean K, Cap Sigma K), where cap sigma K is the covariance matrix.

QDA classifier involves pluggin estimates of cap sigma, mean, and prob(1) in to Quadratic form of the LDA equation. 

```{r }
library(caTools)
set.seed(123)
split = sample.split(dataset$bad, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

library(MASS)
qda.fit = lda(bad~ ., data = training_set)

# Predicting the Test set results
qda.pred = as.data.frame(predict(lda.fit,test_set[-8]))
y_pred_qda = qda.pred$class

# Making the Confusion Matrix
cm = table(test_set[,8], qda.pred$class)
cm
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
print("the accuracy is")
print(accuracy)
```

#####5.5 End Notes

The Model performs equal to that of the logistic regression for a value of C = 0.5 in terms of accuracy, but has a slightly ***increased sensitivity***.

