---
title: "Logistic Regression"
author: "Karthick"
date: "31 March 2018"
output: html_document
---

####4.Logistic Regression

#####4.1 Variables used

Based on the Variable selection technique as used before we are using the three values in the final model as given below :

Age

netFamIncome

RES

#####4.2 Model

```{r }
dfFinal <- read.csv("dfFinal.csv")
dfFinal$totDep <- as.factor(dfFinal$totDep)
dfFinal$PHON <- as.factor(dfFinal$PHON)
dfFinal$bad <- as.factor(dfFinal$bad)

library(caTools)
set.seed(123)
split = sample.split(dfFinal$bad, SplitRatio = 0.75)
training_set = subset(dfFinal, split == TRUE)
test_set = subset(dfFinal, split == FALSE)

classifierFS= glm(formula = bad ~ Age+RES+netFamIncome,
                 family = binomial,
                 data = training_set)
summary(classifierFS)


prob_pred = predict(classifierFS, type = 'response', newdata = test_set[-8])
yPredFS = ifelse(prob_pred > 0.5, 1,0)
y_pred_lrm = ifelse(prob_pred > 0.5, 1,0)
cm = table(test_set[, 8],yPredFS)

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
sensitivity=(cm[2,2]/(cm[2,2]+cm[2,1]))
specificity=(cm[1,1]/(cm[1,1]+cm[1,2]))

library(pROC)
plot.roc(test_set[,8],prob_pred,print.auc=TRUE)

results <- cbind(c("Reduced",as.numeric(AIC(classifierFS)),auc(test_set[, 8], yPredFS ),sensitivity,specificity,accuracy))

```

The type="response" option tells R to ouput probabilities of the form P(Y = 1| X), as opposed to other information such as the logit.

Inorder to predict we must convert these probabilities into class labels, 0 or 1. Hence we use the 

yPredFS = ifelse(prob_pred > 0.5, 1,0)

command to create the prediction with 2 levels. Then form the confusion matrix from which we can calculate accuracy and other related values.

For our model, after testing on the test data, below are the CM matrix and other relevant information.

```{r }
cm


```

 Variable     | $Model 2   | 
--------------|------------|
Type          | Reduced    |
NTerms        | 3          |
AIC           | 1017.73    |    
AUC           | 0.5687     |
Sensitivity   | 0.1728     |
Specificity   | 0.9646     |
Accuracy      | 0.7557     |


#####4.3 Model Explanation

log ( p(X)/(1 ??? p(X))) = beta0 + beta1X1 + betaX2 + beta3X3 + beta4X4 + beta5X5 + beta6X6 

is the model we have built ., The coefficients can be obtained by using the coeff command in R.

 p(X)/(1 ??? p(X)) = exp (beta0 + beta1X1 + betaX2 + beta3X3 + beta4X4 + beta5X5 + beta6X6)
 
 can also be called as odds of the bad credit

```{r  }
classifierFS$coefficients
```

p^(X) = e(beta^0+beta^1X) / (1 + e(beta^0+beta^1X))

is the final probability for the response as discussed in the previous section.The type="response" option tells R to ouput probabilities of the form P(Y = 1| X).

so if the age of the person is 30 and income is 20000 and has "other" as residential status.

Then the probability of him getting a bad credit is 

exp (-1.4772 + 0.02*30 - 2.786x(10^-5)x20000 + 0.998 ) = 0.646

P(bad = 1| X) = ( 0.646/1.646 ) = 0.392

so if the age of the person is 30 and income is 50000 and has "other" as residential status.

Then the probability of him getting a bad credit is 

exp (-1.4772 + 0.02*30 - 2.786*10^-5*50000 + 0.998 ) = 0.2802

P(bad = 1| X) = ( 0.646/1.646 ) = 0.2188

**For increase in income the chance of getting bad credit decreases by 1 time** (exp(-2.79*10^5))

similarly:

**For increase in Age the chance of getting bad credit increase by 1.02 times**(exp(0.02))

**If you are resident of choice "other" the chance of getting bad credit increase by 2.71 times** (exp(0.998))

#####4.4 Cross Validation Sets - 10 Fold

Hold out validation can be said as the simplest "kind of cross-validation", while in k fold cross validation (10 fold here) , we randomly shuffle the dataset into 10 sets d1..d10 , so that all the  sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d1+..+d9 and validate on d10, followed by training on d2+..+d10 and validating on d1 ., untill all the sets are used for validation.

If number of observation = k., then it is exacly same as  leave-one-out cross-validation

```{r, warning=FALSE}

library(caret)
folds = createFolds(training_set$bad, k = 10)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifierLRM = glm(formula = bad ~ Age+netFamIncome + RES,
                      family = binomial,
                      data = training_set)
  prob_pred = predict(classifierLRM, type = 'response', newdata = test_fold[-8])
  y_pred = ifelse(prob_pred > 0.5, 1, 0)
  cm = table(test_fold[, 8], y_pred > 0.5)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
accuracyLRM = mean(as.numeric(cv))
print("Accuracy of the Model after 10 Fold Cross- Validation")
accuracyLRM
```

#####4.5 Visualisation

For the ease of visualisation., i am considering only the 2 variables Age and NetFamilyIncome, for which the performance is more or less same as discussed in the previous sections.

```{r }
# Visualising the Test set results

vis_test_set <- as.data.frame(cbind(test_set$Age,test_set$netFamIncome,test_set$bad))
vis_train_set <- as.data.frame(cbind(training_set$Age,training_set$netFamIncome,training_set$bad))
classifierFS= glm(formula = bad ~ Age+netFamIncome,
                 family = binomial,
                 data = training_set)

library(ElemStatLearn)
set = vis_test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 2)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 1000)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'netFamIncome')
prob_set = predict(classifierFS, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
     main = 'Logistic Regression (Test set)',
     xlab = 'Age', ylab = 'Net Family Income',
     xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))

```

#####4.6 End Notes

The model is clearly predictiong the zero's well ., but that is not the case for 1's/.

Hence Sensitivity = 0.1728  is very low compared to the Specificity = 0.9646 which is very high.

```{r }
plot.roc(test_set[,8],prob_pred,print.auc=TRUE)
```

According to the ROC of the curve ., for higher sensitivity :

the C = 0 lies in the top right corner
c=1 lies in the bottom left corner

hence choosing a value of c = 0.25

```{r, echo=FALSE}
library(caTools)
set.seed(123)
split = sample.split(dfFinal$bad, SplitRatio = 0.755)
training_set = subset(dfFinal, split == TRUE)
test_set = subset(dfFinal, split == FALSE)

classifierFS= glm(formula = bad ~ Age+RES+netFamIncome,
                 family = binomial,
                 data = training_set)
prob_pred = predict(classifierFS, type = 'response', newdata = test_set[-8])
yPredFS = ifelse(prob_pred > 0.260, 1,0)

cm = table(test_set[, 8],yPredFS)

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
sensitivity=(cm[2,2]/(cm[2,2]+cm[2,1]))
specificity=(cm[1,1]/(cm[1,1]+cm[1,2]))
print("The sensitivity of the model is")
sensitivity
print("The accuracy is ")
accuracy
print("The Specificity is ")
specificity
print("The confusion matrix is ")
cm
```

>***Sensitivity of the model can be increased but at the cost of Accuracy and Specificity***
> The optimum level of C is 0.262